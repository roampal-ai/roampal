"""
Runtime Model Switching API
Allows hot-swapping of models without restart
"""
import os
import re
import subprocess
import logging
import asyncio
import json
import time
from pathlib import Path
from typing import List, Dict, Any, AsyncGenerator, Optional
from fastapi import APIRouter, HTTPException, Request, Body, WebSocket, WebSocketDisconnect
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import httpx

logger = logging.getLogger(__name__)
router = APIRouter(tags=["model-switcher"])

# Global state for concurrency control
_download_lock = asyncio.Lock()
_downloading_models: set = set()
_cancel_flags: Dict[str, bool] = {}  # Track which downloads should be cancelled
_env_file_lock = asyncio.Lock()

# Multi-provider configuration
PROVIDERS = {
    "ollama": {"port": 11434, "health": "/api/tags", "models": "/api/tags", "api_style": "ollama"},
    "lmstudio": {"port": 1234, "health": "/v1/models", "models": "/v1/models", "api_style": "openai"},
}

# GGUF model mapping for LM Studio downloads
MODEL_TO_HUGGINGFACE = {
    "qwen2.5:7b": {
        "repo": "bartowski/Qwen2.5-7B-Instruct-GGUF",
        "file": "Qwen2.5-7B-Instruct-Q4_K_M.gguf",
        "size_gb": 4.68
    },
    "llama3.2:3b": {
        "repo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
        "file": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
        "size_gb": 2.0
    },
    "llama3.1:8b": {
        "repo": "bartowski/Meta-Llama-3.1-8B-Instruct-GGUF",
        "file": "Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf",
        "size_gb": 4.9
    },
    "qwen2.5:14b": {
        "repo": "bartowski/Qwen2.5-14B-Instruct-GGUF",
        "file": "Qwen2.5-14B-Instruct-Q4_K_M.gguf",
        "size_gb": 8.99
    },
    "mixtral:8x7b": {
        "repo": "TheBloke/Mixtral-8x7B-Instruct-v0.1-GGUF",
        "file": "mixtral-8x7b-instruct-v0.1.Q4_K_M.gguf",
        "size_gb": 26.44
    },
    "llama3.3:70b": {
        "repo": "bartowski/Llama-3.3-70B-Instruct-GGUF",
        "file": "Llama-3.3-70B-Instruct-Q4_K_M.gguf",
        "size_gb": 42.5
    },
    "qwen2.5:72b": {
        "repo": "bartowski/Qwen2.5-72B-Instruct-GGUF",
        "file": "Qwen2.5-72B-Instruct-Q4_K_M.gguf",
        "size_gb": 47.4
    },
    "qwen2.5:32b": {
        "repo": "bartowski/Qwen2.5-32B-Instruct-GGUF",
        "file": "Qwen2.5-32B-Instruct-Q4_K_M.gguf",
        "size_gb": 19.9
    },
    "qwen2.5:3b": {
        "repo": "bartowski/Qwen2.5-3B-Instruct-GGUF",
        "file": "Qwen2.5-3B-Instruct-Q4_K_M.gguf",
        "size_gb": 1.93
    },
    # gpt-oss models should be downloaded via Ollama (native MXFP4 support)
    # The GGUF versions don't quantize well - bartowski does not recommend them
}

async def detect_provider(name: str, config: dict) -> Optional[dict]:
    """Detect if provider is running"""
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            url = f"http://localhost:{config['port']}{config['health']}"
            response = await client.get(url)
            if response.status_code == 200:
                return {
                    "name": name,
                    "port": config['port'],
                    "url": f"http://localhost:{config['port']}",
                    "api_style": config['api_style'],
                    "status": "running"
                }
    except Exception as e:
        logger.debug(f"Provider {name} not detected on port {config['port']}: {e}")
    return None

async def get_provider_models(name: str, config: dict) -> List[str]:
    """Get models from provider by querying their API"""
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            url = f"http://localhost:{config['port']}{config['models']}"
            response = await client.get(url)
            response.raise_for_status()
            data = response.json()

            if config['api_style'] == 'ollama':
                # Ollama format: {"models": [{"name": "..."}, ...]}
                return [m['name'] for m in data.get('models', [])]
            else:
                # OpenAI format (LM Studio): {"data": [{"id": "..."}, ...]}
                return [m['id'] for m in data.get('data', []) if not m['id'].startswith('text-embedding')]
    except Exception as e:
        logger.error(f"Error fetching models from {name}: {e}")
        return []

def get_lmstudio_cache_path() -> Path:
    """Get LM Studio cache directory for GGUF downloads"""
    cache_dir = Path.home() / ".cache" / "lm-studio" / "models"
    cache_dir.mkdir(parents=True, exist_ok=True)
    return cache_dir

def get_lms_cli_path() -> Optional[Path]:
    """Find lms.exe CLI tool"""
    lms_path = Path.home() / ".lmstudio" / "bin" / "lms.exe"
    if lms_path.exists():
        return lms_path
    # Try alternate Windows location
    appdata = os.getenv('LOCALAPPDATA')
    if appdata:
        alt_path = Path(appdata) / "LM Studio" / "bin" / "lms.exe"
        if alt_path.exists():
            return alt_path
    return None

def format_bytes(bytes_val: int) -> str:
    """Format bytes to human-readable size"""
    for unit in ['B', 'KB', 'MB', 'GB']:
        if bytes_val < 1024.0:
            return f"{bytes_val:.2f} {unit}"
        bytes_val /= 1024.0
    return f"{bytes_val:.2f} TB"

class ModelSwitchRequest(BaseModel):
    model_name: str

def _validate_model_name(model_name: str) -> bool:
    """
    Validate model name format to prevent command injection.
    Valid format: name:tag (e.g., 'qwen3:8b', 'llama2:13b-chat')
    """
    if not model_name or len(model_name) > 100:
        return False
    # Allow alphanumeric, underscore, hyphen, dot, colon
    pattern = r'^[a-zA-Z0-9_.-]+:[a-zA-Z0-9_.-]+$'
    return bool(re.match(pattern, model_name))

def _format_size(bytes_size: int) -> str:
    """Format byte size in human-readable format"""
    if bytes_size == 0:
        return "0 B"
    for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
        if bytes_size < 1024.0:
            return f"{bytes_size:.1f} {unit}"
        bytes_size /= 1024.0
    return f"{bytes_size:.1f} PB"

@router.get("/ollama/status")
async def check_ollama_status():
    """Check if Ollama is installed and running"""
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get('http://localhost:11434/api/tags')
            if response.status_code == 200:
                return {"available": True, "running": True}
            return {"available": False, "running": False, "reason": "Ollama not responding"}
    except (httpx.ConnectError, httpx.TimeoutException):
        return {"available": False, "running": False, "reason": "Ollama not running or not installed"}
    except Exception as e:
        logger.error(f"Error checking Ollama status: {e}")
        return {"available": False, "running": False, "reason": str(e)}

@router.get("/lmstudio/status")
async def check_lmstudio_status():
    """Check if LM Studio server is running"""
    try:
        async with httpx.AsyncClient(timeout=2.0) as client:
            response = await client.get('http://localhost:1234/v1/models')
            if response.status_code == 200:
                return {"available": True, "running": True}
            return {"available": False, "running": False, "reason": "LM Studio not responding"}
    except (httpx.ConnectError, httpx.TimeoutException):
        return {"available": False, "running": False, "reason": "LM Studio server not running"}
    except Exception as e:
        logger.error(f"Error checking LM Studio status: {e}")
        return {"available": False, "running": False, "reason": str(e)}

@router.get("/providers/detect")
async def detect_providers():
    """Detect all running LLM providers"""
    detected = []

    for name, config in PROVIDERS.items():
        provider_info = await detect_provider(name, config)
        if provider_info:
            # Get models for this provider
            models = await get_provider_models(name, config)
            provider_info['models'] = models
            provider_info['model_count'] = len(models)
            detected.append(provider_info)
            logger.info(f"âœ“ Detected {name} on port {config['port']} with {len(models)} models")

    # Get current active provider from environment
    active_provider = os.getenv('ROAMPAL_LLM_PROVIDER', 'ollama')

    return {
        "providers": detected,
        "active": active_provider,
        "count": len(detected)
    }

@router.get("/providers/all/models")
async def list_all_provider_models():
    """Get models from ALL detected providers"""
    all_models = {}

    for name, config in PROVIDERS.items():
        provider_info = await detect_provider(name, config)
        if provider_info:
            models = await get_provider_models(name, config)
            all_models[name] = models
        else:
            all_models[name] = []

    return {"providers": all_models}

@router.get("/providers/{provider_name}/models")
async def list_provider_models(provider_name: str):
    """Get models for a specific provider"""
    if provider_name not in PROVIDERS:
        raise HTTPException(400, f"Unknown provider: {provider_name}. Supported: {list(PROVIDERS.keys())}")

    config = PROVIDERS[provider_name]

    # Check if provider is running
    provider_info = await detect_provider(provider_name, config)
    if not provider_info:
        raise HTTPException(503, f"Provider '{provider_name}' is not running on port {config['port']}")

    models = await get_provider_models(provider_name, config)

    return {
        "provider": provider_name,
        "models": models,
        "count": len(models)
    }

@router.post("/download-gguf-stream")
async def download_gguf_stream(request_body: Dict[str, Any] = Body(...)):
    """Download GGUF model from HuggingFace and import to LM Studio"""

    model_name = request_body.get('model', '')

    # Validate model exists in mapping
    if model_name not in MODEL_TO_HUGGINGFACE:
        raise HTTPException(404, f"Model {model_name} not available for LM Studio auto-download")

    async def generate():
        # Get model info inside generator (closure scope issue)
        model_info = MODEL_TO_HUGGINGFACE[model_name]

        # Acquire download lock INSIDE generator to ensure it's held during the actual download
        logger.info(f"[LOCK DEBUG] Generator started, attempting to acquire lock for {model_name}")
        async with _download_lock:
            logger.info(f"[LOCK DEBUG] Lock acquired for {model_name}, current downloads: {_downloading_models}")
            # Only allow one download at a time (any model)
            if _downloading_models:
                current = next(iter(_downloading_models))
                logger.info(f"[LOCK DEBUG] Blocking {model_name} - {current} is already downloading")
                yield f"data: {json.dumps({'type': 'error', 'message': f'Cannot download {model_name}: {current} is already downloading'})}\n\n"
                return
            if model_name in _downloading_models:
                logger.info(f"[LOCK DEBUG] Blocking {model_name} - already in download set")
                yield f"data: {json.dumps({'type': 'error', 'message': f'Model {model_name} is already being downloaded'})}\n\n"
                return
            _downloading_models.add(model_name)
            logger.info(f"[LOCK DEBUG] Added {model_name} to download set, releasing lock. Set: {_downloading_models}")

        logger.info(f"[LOCK DEBUG] Lock released, starting download for {model_name}, set ID: {id(_downloading_models)}")
        try:
            # 1. Download GGUF file from HuggingFace
            cache_path = get_lmstudio_cache_path() / model_info['file']
            hf_url = f"https://huggingface.co/{model_info['repo']}/resolve/main/{model_info['file']}"

            logger.info(f"Downloading {model_name} from {hf_url}")

            yield f"data: {json.dumps({'type': 'start', 'model': model_name})}\n\n"
            yield f"data: {json.dumps({'type': 'progress', 'status': 'downloading', 'progress': 0, 'message': 'Starting download...'})}\n\n"

            async with httpx.AsyncClient(timeout=600.0, follow_redirects=True) as client:
                async with client.stream('GET', hf_url) as response:
                    response.raise_for_status()
                    total = int(response.headers.get('content-length', 0))
                    downloaded = 0
                    last_progress = 0
                    download_start_time = time.time()
                    bytes_at_last_update = 0
                    last_update_time = download_start_time

                    with open(cache_path, 'wb') as f:
                        async for chunk in response.aiter_bytes(chunk_size=8192):
                            # Check if download was cancelled
                            if _cancel_flags.get(model_name, False):
                                logger.info(f"Download cancelled for {model_name}")
                                raise Exception("Download cancelled by user")

                            f.write(chunk)
                            downloaded += len(chunk)

                            if total > 0:
                                progress = int((downloaded / total) * 100)
                                # Yield every 1% for smooth progress (same as Ollama)
                                if progress >= last_progress + 1 or progress == 100:
                                    downloaded_str = format_bytes(downloaded)
                                    total_str = format_bytes(total)

                                    # Calculate download speed
                                    current_time = time.time()
                                    time_elapsed = current_time - last_update_time
                                    if time_elapsed > 0:
                                        bytes_diff = downloaded - bytes_at_last_update
                                        speed_bps = bytes_diff / time_elapsed
                                        speed_str = f"{format_bytes(int(speed_bps))}/s"
                                    else:
                                        speed_str = "calculating..."

                                    bytes_at_last_update = downloaded
                                    last_update_time = current_time

                                    yield f"data: {json.dumps({'type': 'progress', 'status': 'downloading', 'percent': progress, 'downloaded': downloaded_str, 'total': total_str, 'speed': speed_str, 'message': f'Downloading {progress}%'})}\n\n"
                                    last_progress = progress

            logger.info(f"Downloaded {model_name} to {cache_path}")

            # 2. Import via lms.exe
            yield f"data: {json.dumps({'type': 'progress', 'status': 'importing', 'progress': 0, 'message': 'Importing to LM Studio...'})}\n\n"

            lms_cli = get_lms_cli_path()
            if not lms_cli:
                raise Exception("lms.exe not found. Is LM Studio installed?")

            logger.info(f"Importing with lms.exe: {lms_cli}")

            # Use Popen with stdin PIPE to answer first-run prompt
            process = subprocess.Popen([
                str(lms_cli), "import",
                "--yes",
                "--copy",
                "--user-repo", model_info['repo'],
                str(cache_path)
            ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

            # Send Y to answer first-run prompt, then close stdin
            if process.stdin:
                process.stdin.write("Y\n")
                process.stdin.flush()
                process.stdin.close()

            # Read output in real-time with heartbeat updates
            stdout_lines = []
            stderr_lines = []
            last_heartbeat = 0

            while True:
                # Check if process is done
                retcode = process.poll()
                if retcode is not None:
                    # Process finished - read any remaining output
                    remaining_out = process.stdout.read() if process.stdout else ""
                    remaining_err = process.stderr.read() if process.stderr else ""
                    if remaining_out:
                        stdout_lines.append(remaining_out)
                    if remaining_err:
                        stderr_lines.append(remaining_err)
                    break

                # Send heartbeat every 5 seconds to show it's still working
                current_time = time.time()
                if current_time - last_heartbeat >= 5:
                    yield f"data: {json.dumps({'type': 'progress', 'status': 'importing', 'progress': 0, 'message': 'Importing to LM Studio (this may take a minute)...'})}\n\n"
                    last_heartbeat = current_time

                # Small sleep to avoid busy-waiting
                await asyncio.sleep(0.5)

            stdout = "".join(stdout_lines)
            stderr = "".join(stderr_lines)

            # Create result object to match expected interface
            class Result:
                def __init__(self, returncode, stdout, stderr):
                    self.returncode = returncode
                    self.stdout = stdout
                    self.stderr = stderr

            result = Result(retcode, stdout, stderr)

            # Check if file was actually imported (don't trust stderr)
            imported_file_path = Path.home() / ".lmstudio" / "models" / model_info['repo'].split('/')[0] / model_info['repo'].split('/')[1] / model_info['file']

            if not imported_file_path.exists():
                logger.error(f"lms import failed - file not found at: {imported_file_path}")
                logger.error(f"stdout: {result.stdout}")
                logger.error(f"stderr: {result.stderr}")
                raise Exception(f"Import failed: Model file not found in LM Studio models directory")

            logger.info(f"Successfully imported {model_name} to {imported_file_path}")

            # Delete cache file after successful import
            try:
                cache_path.unlink()
                logger.info(f"Deleted cache file: {cache_path}")
            except Exception as e:
                logger.warning(f"Could not delete cache file {cache_path}: {e}")

            # Query LM Studio API to get the actual model ID after import
            actual_model_id = None
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get('http://localhost:1234/v1/models')
                    if response.status_code == 200:
                        data = response.json()
                        lmstudio_models = [m.get('id', '') for m in data.get('data', [])]
                        logger.info(f"LM Studio models after import: {lmstudio_models}")

                        # Try multiple matching strategies to find the imported model
                        # Strategy 1: Direct match on original model name with hyphens (qwen2.5:7b -> qwen2.5-7b)
                        normalized_name = model_name.replace(':', '-')
                        for model_id in lmstudio_models:
                            if normalized_name in model_id or model_id.startswith(normalized_name):
                                actual_model_id = model_id
                                logger.info(f"Found LM Studio model ID via normalized match: {actual_model_id}")
                                break

                        # Strategy 2: Match on repo name parts (e.g., qwen2.5, 7b, instruct)
                        if not actual_model_id:
                            # Extract key parts from model_name (e.g., qwen2.5:7b -> ["qwen2", "5", "7b"])
                            name_parts = model_name.replace(':', '.').replace('-', '.').split('.')
                            for model_id in lmstudio_models:
                                model_id_lower = model_id.lower()
                                # Check if all key parts appear in model_id
                                if all(part.lower() in model_id_lower for part in name_parts if part):
                                    actual_model_id = model_id
                                    logger.info(f"Found LM Studio model ID via part matching: {actual_model_id}")
                                    break
            except Exception as e:
                logger.warning(f"Failed to query LM Studio for actual model ID: {e}")

            # Use actual model ID if found, otherwise fall back to original name
            model_to_return = actual_model_id if actual_model_id else model_name
            logger.info(f"Successfully imported {model_name} - ready for use as '{model_to_return}'")
            yield f"data: {json.dumps({'type': 'loaded', 'model': model_to_return, 'message': 'Model ready!'})}\n\n"

        except httpx.ConnectError as e:
            logger.error(f"Connection failed for {model_name}: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': 'Cannot connect to HuggingFace. Check your internet connection.'})}\n\n"
        except httpx.TimeoutException as e:
            logger.error(f"Download timeout for {model_name}: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': f'Download timed out for {model_name}. Try again or check your connection.'})}\n\n"
        except OSError as e:
            if "No space left" in str(e) or "not enough space" in str(e).lower():
                model_info = MODEL_TO_HUGGINGFACE.get(model_name, {})
                size_gb = model_info.get('size_gb', 'unknown')
                logger.error(f"Disk space error for {model_name}: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': f'Not enough disk space to download {model_name} ({size_gb}GB required)'})}\n\n"
            else:
                logger.error(f"Disk error for {model_name}: {e}")
                yield f"data: {json.dumps({'type': 'error', 'message': f'Disk error: {str(e)}'})}\n\n"
        except PermissionError as e:
            logger.error(f"Permission error for {model_name}: {e}")
            yield f"data: {json.dumps({'type': 'error', 'message': 'Permission denied. Try running as administrator or check folder permissions.'})}\n\n"
        except Exception as e:
            error_msg = str(e)
            if "cancelled" in error_msg.lower():
                logger.info(f"Download cancelled for {model_name}")
                yield f"data: {json.dumps({'type': 'error', 'message': f'Download cancelled for {model_name}'})}\n\n"
            else:
                logger.error(f"GGUF download/import failed for {model_name}: {e}", exc_info=True)
                yield f"data: {json.dumps({'type': 'error', 'message': f'Unexpected error: {str(e)}'})}\n\n"
        finally:
            logger.info(f"[LOCK DEBUG] Cleaning up {model_name} from download set. Set before: {_downloading_models}")
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)  # Clean up cancel flag
            logger.info(f"[LOCK DEBUG] Cleaned up {model_name}. Set after: {_downloading_models}")

    return StreamingResponse(generate(), media_type="text/event-stream")

@router.post("/cancel-download")
async def cancel_download(request_body: Dict[str, Any] = Body(...)):
    """Cancel ongoing GGUF download and cleanup"""
    model_name = request_body.get('model', '')

    if model_name in _downloading_models:
        # Set cancel flag to stop the download loop
        _cancel_flags[model_name] = True
        logger.info(f"Cancel flag set for {model_name}")

        # Wait a moment for the download to stop
        await asyncio.sleep(0.5)

        # Remove from downloading set
        _downloading_models.discard(model_name)

        # Try to delete partial download if it exists
        if model_name in MODEL_TO_HUGGINGFACE:
            model_info = MODEL_TO_HUGGINGFACE[model_name]
            cache_path = get_lmstudio_cache_path() / model_info['file']
            if cache_path.exists():
                try:
                    cache_path.unlink()
                    logger.info(f"Deleted partial download for {model_name}")
                except Exception as e:
                    logger.warning(f"Could not delete partial file {cache_path}: {e}")

        return {"status": "cancelled", "model": model_name}

    return {"status": "not_found", "model": model_name}

def _map_lmstudio_id_to_original_name(lmstudio_id: str) -> str:
    """Map LM Studio model ID back to original download name (e.g., qwen2.5-7b-instruct -> qwen2.5:7b)"""
    # Try direct mapping first by normalizing (qwen2.5:7b -> qwen2.5-7b)
    for original_name in MODEL_TO_HUGGINGFACE.keys():
        normalized = original_name.replace(':', '-')
        if normalized in lmstudio_id or lmstudio_id.startswith(normalized):
            return original_name

    # Fallback: try matching by key parts (qwen2.5, 7b, etc.)
    for original_name in MODEL_TO_HUGGINGFACE.keys():
        name_parts = original_name.replace(':', '.').replace('-', '.').split('.')
        lmstudio_lower = lmstudio_id.lower()
        if all(part.lower() in lmstudio_lower for part in name_parts if part):
            return original_name

    # If no match, return the LM Studio ID as-is
    return lmstudio_id

@router.get("/available")
async def list_available_models():
    """List all locally available models from both Ollama and LM Studio"""
    all_models = []

    # 1. Get Ollama models
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get('http://localhost:11434/api/tags')
            if response.status_code == 200:
                data = response.json()
                for model in data.get('models', []):
                    all_models.append({
                        "name": model.get('name', ''),
                        "id": model.get('digest', '')[:12],
                        "size": _format_size(model.get('size', 0)),
                        "modified": model.get('modified_at', ''),
                        "provider": "ollama"
                    })
                logger.info(f"Found {len(all_models)} Ollama models")
    except Exception as e:
        logger.warning(f"Failed to fetch Ollama models: {e}")

    # 2. Get LM Studio models
    try:
        async with httpx.AsyncClient(timeout=5.0) as client:
            response = await client.get('http://localhost:1234/v1/models')
            if response.status_code == 200:
                data = response.json()
                lmstudio_count = 0
                for model in data.get('data', []):
                    model_id = model.get('id', '')
                    if model_id and model_id != 'text-embedding-nomic-embed-text-v1.5':  # Skip embedding models
                        # Map LM Studio ID back to original name for UI matching
                        original_name = _map_lmstudio_id_to_original_name(model_id)
                        all_models.append({
                            "name": original_name,  # Use original name (qwen2.5:7b) for UI matching
                            "id": model_id[:12] if len(model_id) > 12 else model_id,
                            "size": "N/A",  # LM Studio API doesn't provide size easily
                            "modified": "",
                            "provider": "lmstudio",
                            "lmstudio_id": model_id  # Store actual LM Studio ID for switching
                        })
                        lmstudio_count += 1
                logger.info(f"Found {lmstudio_count} LM Studio models")
    except Exception as e:
        logger.warning(f"Failed to fetch LM Studio models: {e}")

    return {"models": all_models, "count": len(all_models)}

@router.get("/current")
async def get_current_model(request: Request):
    """Get the currently active model"""
    # ALWAYS prioritize runtime state over env vars (runtime is source of truth)
    current_model = None
    llm_client_available = False

    if hasattr(request.app.state, 'llm_client') and request.app.state.llm_client is not None:
        if hasattr(request.app.state.llm_client, 'model_name'):
            current_model = request.app.state.llm_client.model_name
            llm_client_available = True
            logger.info(f"Current model from runtime: {current_model}")

    # Fallback to env var only if no runtime client
    if not current_model:
        current_model = os.getenv('ROAMPAL_LLM_OLLAMA_MODEL') or os.getenv('OLLAMA_MODEL') or 'codellama'
        logger.info(f"Current model from env fallback: {current_model}")

    # CRITICAL: Verify model actually exists in its provider before saying it's active
    # Use llm_client's api_style as source of truth - it knows which provider it's configured for
    model_exists = False
    if current_model:
        checking_provider = None
        if hasattr(request.app.state, 'llm_client') and request.app.state.llm_client is not None:
            if hasattr(request.app.state.llm_client, 'api_style'):
                api_style = request.app.state.llm_client.api_style
                checking_provider = 'lmstudio' if api_style == 'openai' else 'ollama'
            elif hasattr(request.app.state.llm_client, 'base_url'):
                base_url = request.app.state.llm_client.base_url
                checking_provider = 'lmstudio' if '1234' in base_url else 'ollama'

        # Check the appropriate provider
        if checking_provider == 'ollama':
            try:
                result = subprocess.run(
                    ["ollama", "list"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    available_models = [line.split()[0] for line in result.stdout.strip().split('\n')[1:] if line.strip()]
                    model_exists = current_model in available_models
                    if not model_exists:
                        logger.warning(f"Model {current_model} not found in Ollama")
            except Exception as e:
                logger.error(f"Failed to verify model in Ollama: {e}")

        elif checking_provider == 'lmstudio':
            try:
                async with httpx.AsyncClient(timeout=2.0) as client:
                    response = await client.get("http://localhost:1234/v1/models")
                    if response.status_code == 200:
                        data = response.json()
                        available_models = [m['id'] for m in data.get('data', [])]
                        model_exists = current_model in available_models
                        if not model_exists:
                            logger.warning(f"Model {current_model} not found in LM Studio")
            except Exception as e:
                logger.error(f"Failed to verify model in LM Studio: {e}")

        # If model doesn't exist in its provider, clear it
        if not model_exists:
            current_model = None

    # Check if current model is an embedding model or None
    embedding_models = ['nomic-embed-text', 'mxbai-embed-large', 'all-minilm']
    is_embedding = current_model and any(embed in current_model.lower() for embed in embedding_models)
    # Only can_chat if: llm_client exists AND model exists AND not embedding
    can_chat = llm_client_available and current_model and not is_embedding and model_exists

    # Get current provider from llm_client
    current_provider = None
    if hasattr(request.app.state, 'llm_client') and request.app.state.llm_client is not None:
        if hasattr(request.app.state.llm_client, 'api_style'):
            api_style = request.app.state.llm_client.api_style
            # Map api_style to provider name
            current_provider = 'lmstudio' if api_style == 'openai' else 'ollama'
        elif hasattr(request.app.state.llm_client, 'base_url'):
            # Fallback: detect from base_url
            base_url = request.app.state.llm_client.base_url
            current_provider = 'lmstudio' if '1234' in base_url else 'ollama'

    return {
        "current_model": current_model if (llm_client_available and model_exists) else None,
        "can_chat": can_chat,
        "is_embedding_model": is_embedding,
        "provider": current_provider
    }

@router.post("/switch")
async def switch_model(request: Request, model_request: ModelSwitchRequest):
    """Switch to a different model at runtime - supports both Ollama and LM Studio"""
    try:
        model_name = model_request.model_name

        # Step 1: Detect which provider owns this model
        provider = None
        base_url = None
        api_style = None

        # Check Ollama
        try:
            import shutil
            ollama_cmd = shutil.which("ollama")
            if not ollama_cmd:
                common_paths = [
                    r"C:\Users\{}\AppData\Local\Programs\Ollama\ollama.exe".format(os.environ.get('USERNAME', '')),
                    r"C:\Program Files\Ollama\ollama.exe",
                ]
                for path in common_paths:
                    if os.path.exists(path):
                        ollama_cmd = path
                        break

            if ollama_cmd:
                result = subprocess.run(
                    [ollama_cmd, "list"],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                if result.returncode == 0:
                    available_models = [line.split()[0] for line in result.stdout.strip().split('\n')[1:] if line.strip()]
                    if model_name in available_models:
                        provider = 'ollama'
                        base_url = 'http://localhost:11434'
                        api_style = 'ollama'
                        logger.info(f"Model '{model_name}' found in Ollama")
        except Exception as e:
            logger.warning(f"Failed to check Ollama: {e}")

        # Check LM Studio if not found in Ollama
        if not provider:
            try:
                async with httpx.AsyncClient(timeout=5.0) as client:
                    response = await client.get('http://localhost:1234/v1/models')
                    if response.status_code == 200:
                        data = response.json()
                        lmstudio_models = [model.get('id', '') for model in data.get('data', [])]
                        if model_name in lmstudio_models:
                            provider = 'lmstudio'
                            base_url = 'http://localhost:1234'
                            api_style = 'openai'
                            logger.info(f"Model '{model_name}' found in LM Studio")
            except Exception as e:
                logger.warning(f"Failed to check LM Studio: {e}")

        # If model not found in either provider, raise error
        if not provider:
            raise HTTPException(
                status_code=404,
                detail=f"Model '{model_name}' not found in Ollama or LM Studio"
            )

        # Step 2: Update the LLM client
        if hasattr(request.app.state, 'llm_client'):
            # Lazy initialization: create LLM client if None
            if request.app.state.llm_client is None:
                from modules.llm.ollama_client import OllamaClient
                request.app.state.llm_client = OllamaClient()
                await request.app.state.llm_client.initialize({"ollama_model": model_name})
                logger.info(f"Lazily initialized LLM client with model: {model_name}")
                previous_model = None
                previous_provider = None
            else:
                # Store previous model and provider for rollback
                previous_model = request.app.state.llm_client.model_name
                previous_provider = getattr(request.app.state.llm_client, 'api_style', 'ollama')

                # Update llm_client with new provider configuration
                request.app.state.llm_client.model_name = model_name
                request.app.state.llm_client.base_url = base_url
                request.app.state.llm_client.api_style = api_style
                logger.info(f"Updated llm_client: model={model_name}, provider={provider}, base_url={base_url}")

            # Update global agent_service LLM reference
            from app.routers.agent_chat import agent_service
            if agent_service and hasattr(agent_service, 'llm'):
                agent_service.llm = request.app.state.llm_client

            # Step 3: Run health check against correct provider endpoint
            try:
                logger.info(f"Performing health check on {provider} model: {model_name}")

                # Recycle HTTP client if available
                if hasattr(request.app.state.llm_client, '_recycle_client'):
                    await request.app.state.llm_client._recycle_client()

                await asyncio.sleep(2.0)  # Give provider time to load model

                async with httpx.AsyncClient(timeout=30.0) as health_client:
                    if provider == 'ollama':
                        health_payload = {
                            "model": model_name,
                            "messages": [{"role": "user", "content": "test"}],
                            "stream": False,
                            "options": {
                                "num_ctx": 2048,
                                "num_predict": 10
                            }
                        }
                        health_response = await health_client.post(
                            "http://localhost:11434/api/chat",
                            json=health_payload
                        )
                    else:  # lmstudio
                        health_payload = {
                            "model": model_name,
                            "messages": [{"role": "user", "content": "test"}],
                            "max_tokens": 10
                        }
                        health_response = await health_client.post(
                            "http://localhost:1234/v1/chat/completions",
                            json=health_payload
                        )

                    health_response.raise_for_status()
                    logger.info(f"Health check passed for {provider} model: {model_name}")

            except Exception as health_error:
                logger.error(f"Health check failed for {model_name}: {health_error}")
                # Rollback
                if previous_model:
                    request.app.state.llm_client.model_name = previous_model
                    if previous_provider == 'lmstudio':
                        request.app.state.llm_client.base_url = 'http://localhost:1234'
                        request.app.state.llm_client.api_style = 'openai'
                    else:
                        request.app.state.llm_client.base_url = 'http://localhost:11434'
                        request.app.state.llm_client.api_style = 'ollama'
                raise HTTPException(
                    status_code=503,
                    detail=f"Model {model_name} failed health check: {str(health_error)}. Rolled back to {previous_model}"
                )

            # Step 4: Update environment variables
            os.environ['ROAMPAL_LLM_PROVIDER'] = provider
            if provider == 'ollama':
                os.environ['OLLAMA_MODEL'] = model_name
                os.environ['ROAMPAL_LLM_OLLAMA_MODEL'] = model_name
            else:
                os.environ['ROAMPAL_LLM_LMSTUDIO_MODEL'] = model_name

            # Update .env file
            async with _env_file_lock:
                env_path = Path(__file__).parent.parent.parent / '.env'
                if env_path.exists():
                    lines = env_path.read_text().splitlines()

                    # Update ROAMPAL_LLM_PROVIDER
                    provider_updated = False
                    for i, line in enumerate(lines):
                        if line.startswith('ROAMPAL_LLM_PROVIDER='):
                            lines[i] = f'ROAMPAL_LLM_PROVIDER={provider}'
                            provider_updated = True
                            break
                    if not provider_updated:
                        lines.append(f'ROAMPAL_LLM_PROVIDER={provider}')

                    # Update model-specific env vars
                    if provider == 'ollama':
                        for i, line in enumerate(lines):
                            if line.startswith('OLLAMA_MODEL='):
                                lines[i] = f'OLLAMA_MODEL={model_name}'
                            elif line.startswith('ROAMPAL_LLM_OLLAMA_MODEL='):
                                lines[i] = f'ROAMPAL_LLM_OLLAMA_MODEL={model_name}'
                    else:
                        lmstudio_updated = False
                        for i, line in enumerate(lines):
                            if line.startswith('ROAMPAL_LLM_LMSTUDIO_MODEL='):
                                lines[i] = f'ROAMPAL_LLM_LMSTUDIO_MODEL={model_name}'
                                lmstudio_updated = True
                                break
                        if not lmstudio_updated:
                            lines.append(f'ROAMPAL_LLM_LMSTUDIO_MODEL={model_name}')

                    env_path.write_text('\n'.join(lines) + '\n')

            return {
                "status": "success",
                "message": f"Switched to {provider} model: {model_name}",
                "current_model": model_name,
                "provider": provider
            }
        else:
            raise HTTPException(
                status_code=503,
                detail="LLM client not initialized"
            )
    except HTTPException:
        raise
    except Exception as e:
        import traceback
        logger.error(f"Error switching model: {e}")
        logger.error(f"Traceback: {traceback.format_exc()}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to switch model: {str(e)}"
        )

@router.post("/pull")
async def pull_model(request_body: Dict[str, Any] = Body(...)):
    """Download a new model from Ollama registry"""
    try:
        # Extract model name from request body (UI sends 'model', accept both)
        model_name = request_body.get('model') or request_body.get('model_name', '')
        if not model_name:
            raise HTTPException(
                status_code=400,
                detail="Model name is required"
            )

        # Validate model name format
        if not _validate_model_name(model_name):
            raise HTTPException(
                status_code=400,
                detail=f"Invalid model name format. Expected format: name:tag (e.g., 'qwen3:8b')"
            )

        # Check if already downloading
        if model_name in _downloading_models:
            raise HTTPException(
                status_code=409,
                detail=f"Model {model_name} is already being downloaded"
            )

        # Acquire download lock
        async with _download_lock:
            _downloading_models.add(model_name)

        try:
            logger.info(f"Pulling model: {model_name}")
            # Start the pull process
            result = subprocess.run(
                ["ollama", "pull", model_name],
                capture_output=True,
                text=True,
                timeout=600  # 10 minute timeout for large models
            )

            if result.returncode == 0:
                return {
                    "status": "success",
                    "message": f"Successfully pulled {model_name}",
                    "output": result.stdout
                }
            else:
                raise HTTPException(
                    status_code=500,
                    detail=f"Failed to pull model: {result.stderr}"
                )
        finally:
            # Always remove from downloading set and clean up cancel flag
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)

    except subprocess.TimeoutExpired:
        _downloading_models.discard(model_name)
        _cancel_flags.pop(model_name, None)
        raise HTTPException(
            status_code=504,
            detail="Model pull timed out after 10 minutes"
        )
    except HTTPException:
        raise
    except Exception as e:
        _downloading_models.discard(model_name)
        _cancel_flags.pop(model_name, None)
        raise HTTPException(
            status_code=500,
            detail=f"Error pulling model: {str(e)}"
        )

@router.post("/pull-stream")
async def pull_model_stream(request_body: Dict[str, Any] = Body(...)):
    """Download a model with real-time progress streaming"""
    model_name = request_body.get('model') or request_body.get('model_name', '')
    if not model_name:
        raise HTTPException(status_code=400, detail="Model name is required")

    # Validate model name format
    if not _validate_model_name(model_name):
        raise HTTPException(
            status_code=400,
            detail=f"Invalid model name format. Expected format: name:tag (e.g., 'qwen3:8b')"
        )

    # Acquire download lock and check if already downloading (prevents race condition)
    async with _download_lock:
        # Only allow one download at a time (any model)
        if _downloading_models:
            current = next(iter(_downloading_models))
            raise HTTPException(
                status_code=409,
                detail=f"Cannot download {model_name}: {current} is already downloading. Only one download at a time."
            )
        _downloading_models.add(model_name)

    async def generate_progress() -> AsyncGenerator[str, None]:
        """Generate SSE events with download progress using Ollama HTTP API"""
        try:
            yield f"data: {json.dumps({'type': 'start', 'model': model_name})}\n\n"

            # Use Ollama's HTTP API endpoint for pulling models
            # Configure granular timeouts: 60s between data chunks prevents infinite hangs
            async with httpx.AsyncClient(timeout=httpx.Timeout(
                connect=5.0,   # 5s to establish connection
                read=60.0,     # 60s between receiving data chunks (prevents hang)
                write=5.0,     # 5s to send request
                pool=5.0       # 5s to get connection from pool
            )) as client:
                async with client.stream(
                    'POST',
                    'http://localhost:11434/api/pull',
                    json={'name': model_name, 'stream': True}
                ) as response:
                    if response.status_code != 200:
                        error_text = await response.aread()
                        raise Exception(f"Ollama API error: {error_text.decode('utf-8', errors='replace')}")

                    # Process NDJSON stream (one JSON object per line)
                    async for line in response.aiter_lines():
                        # Check if download was cancelled
                        if _cancel_flags.get(model_name, False):
                            logger.info(f"Ollama download cancelled for {model_name}")
                            raise Exception("Download cancelled by user")

                        if not line.strip():
                            continue

                        try:
                            data = json.loads(line)
                            status = data.get('status', '')

                            logger.info(f"Ollama API: {status}")

                            # Check for errors first
                            if 'error' in data:
                                error_msg = data.get('error', 'Unknown error')
                                logger.error(f"Ollama API error: {error_msg}")
                                error_data = {'type': 'error', 'message': f'Download failed: {error_msg}'}
                                yield f"data: {json.dumps(error_data)}\n\n"
                                return  # Exit generator, don't send false success

                            # Map Ollama API responses to SSE events
                            if 'total' in data and 'completed' in data:
                                # Download progress
                                total = data['total']
                                completed = data['completed']
                                percent = int((completed / total * 100)) if total > 0 else 0

                                # Format sizes
                                def format_bytes(b):
                                    for unit in ['B', 'KB', 'MB', 'GB']:
                                        if b < 1024:
                                            return f"{b:.1f} {unit}"
                                        b /= 1024
                                    return f"{b:.1f} TB"

                                downloaded_str = format_bytes(completed)
                                total_str = format_bytes(total)

                                progress_data = {
                                    'type': 'progress',
                                    'percent': percent,
                                    'downloaded': downloaded_str,
                                    'total': total_str,
                                    'speed': 'downloading',
                                    'message': f'{status}: {percent}%'
                                }
                                yield f"data: {json.dumps(progress_data)}\n\n"

                            elif status.startswith('pulling'):
                                # Layer pull in progress
                                yield f"data: {json.dumps({'type': 'progress', 'message': status})}\n\n"

                            elif status == 'verifying sha256 digest':
                                verify_data = {'type': 'progress', 'percent': 100, 'message': 'Verifying model...'}
                                yield f"data: {json.dumps(verify_data)}\n\n"

                            elif status == 'success':
                                # Verify model actually installed before claiming success
                                try:
                                    async with httpx.AsyncClient(timeout=5.0) as verify_client:
                                        verify_response = await verify_client.get('http://localhost:11434/api/tags')
                                        tags_data = verify_response.json()
                                        model_exists = any(m['name'] == model_name for m in tags_data.get('models', []))

                                        if model_exists:
                                            logger.info(f"Model {model_name} downloaded successfully and verified")
                                            complete_data = {'type': 'complete', 'success': True, 'message': f'Successfully installed {model_name}'}
                                            yield f"data: {json.dumps(complete_data)}\n\n"
                                        else:
                                            logger.error(f"Model {model_name} reported success but not found in Ollama")
                                            error_data = {'type': 'error', 'message': f'Download reported success but {model_name} not found in Ollama'}
                                            yield f"data: {json.dumps(error_data)}\n\n"
                                except Exception as verify_error:
                                    logger.error(f"Verification failed: {verify_error}")
                                    error_data = {'type': 'error', 'message': f'Verification failed: {str(verify_error)}'}
                                    yield f"data: {json.dumps(error_data)}\n\n"
                                break

                        except json.JSONDecodeError as e:
                            logger.warning(f"Failed to parse Ollama API response: {line[:100]}")
                            continue

        except Exception as e:
            import traceback
            error_details = traceback.format_exc()
            logger.error(f"Error streaming model pull: {e}\n{error_details}")
            exception_data = {'type': 'error', 'message': f'Error: {str(e)}'}
            yield f"data: {json.dumps(exception_data)}\n\n"
        finally:
            # Always remove from downloading set and clean up cancel flag
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)

    return StreamingResponse(
        generate_progress(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"  # Disable nginx buffering
        }
    )

@router.delete("/uninstall/{model_name:path}")
async def uninstall_model(model_name: str, request: Request, provider_hint: str = None):
    """Uninstall/remove a model from Ollama or LM Studio

    Args:
        model_name: The model name from the UI (registry name like qwen2.5:7b)
        provider_hint: Optional hint from frontend about which provider tab user was viewing
    """
    try:
        # Decode the URL-encoded model name
        import urllib.parse
        model_name = urllib.parse.unquote(model_name)

        logger.info(f"Attempting to uninstall model: {model_name} (provider_hint: {provider_hint})")

        # If provider_hint is given, use it to determine which provider to check first
        # For LM Studio, map registry name to actual model name
        provider = None
        actual_model_name = model_name

        if provider_hint == "lmstudio":
            # Map registry name to LM Studio model name
            if model_name in MODEL_TO_HUGGINGFACE:
                # For LM Studio, the actual model ID is different from registry name
                # Check what's actually installed in LM Studio
                try:
                    async with httpx.AsyncClient(timeout=2.0) as client:
                        resp = await client.get("http://localhost:1234/v1/models")
                        if resp.status_code == 200:
                            lms_models = [m['id'] for m in resp.json().get('data', [])]

                            # Try to find matching model in LM Studio
                            # Registry name: qwen2.5:7b -> LM Studio ID: qwen2.5-7b-instruct
                            normalized_name = model_name.replace(':', '-')
                            for lms_id in lms_models:
                                if normalized_name in lms_id or lms_id.startswith(normalized_name):
                                    provider = "lmstudio"
                                    actual_model_name = lms_id
                                    logger.info(f"Mapped registry name {model_name} to LM Studio model {lms_id}")
                                    break
                except Exception as e:
                    logger.error(f"Failed to check LM Studio for {model_name}: {e}")

        elif provider_hint == "ollama":
            # For Ollama, registry name matches actual model name
            try:
                async with httpx.AsyncClient() as client:
                    resp = await client.get("http://localhost:11434/api/tags", timeout=2.0)
                    if resp.status_code == 200:
                        ollama_models = [m['name'] for m in resp.json().get('models', [])]
                        if model_name in ollama_models:
                            provider = "ollama"
                            actual_model_name = model_name
            except Exception as e:
                logger.error(f"Failed to check Ollama for {model_name}: {e}")

        # If provider_hint didn't find it, fall back to checking both providers
        if not provider:
            logger.info(f"Provider hint didn't find model, checking both providers...")
            # Reset actual_model_name to original since hint failed
            actual_model_name = model_name

            # Check Ollama models
            try:
                async with httpx.AsyncClient() as client:
                    resp = await client.get("http://localhost:11434/api/tags", timeout=2.0)
                    if resp.status_code == 200:
                        ollama_models = [m['name'] for m in resp.json().get('models', [])]
                        if model_name in ollama_models:
                            provider = "ollama"
                            actual_model_name = model_name
            except:
                pass

            # Check LM Studio models
            if not provider:
                try:
                    async with httpx.AsyncClient() as client:
                        resp = await client.get("http://localhost:1234/v1/models", timeout=2.0)
                        if resp.status_code == 200:
                            lms_models = [m['id'] for m in resp.json().get('data', [])]

                            # Try direct match first
                            if model_name in lms_models:
                                provider = "lmstudio"
                                actual_model_name = model_name
                            else:
                                # Try mapping: qwen2.5:7b -> qwen2.5-7b-instruct
                                normalized_name = model_name.replace(':', '-')
                                for lms_id in lms_models:
                                    if normalized_name in lms_id or lms_id.startswith(normalized_name):
                                        provider = "lmstudio"
                                        actual_model_name = lms_id
                                        logger.info(f"Mapped {model_name} to LM Studio ID: {lms_id}")
                                        break
                except:
                    pass

        if not provider:
            logger.error(f"Model {model_name} not found in any provider")
            return {"success": False, "error": "Model not found", "message": f"Model {model_name} not found"}

        # Uninstall based on provider
        if provider == "ollama":
            # Validate model name format for Ollama
            if not _validate_model_name(actual_model_name):
                raise HTTPException(400, f"Invalid Ollama model name format")

            result = subprocess.run(
                ["ollama", "rm", actual_model_name],
                capture_output=True,
                text=True,
                timeout=30
            )
        elif provider == "lmstudio":
            # For LM Studio, find and delete the GGUF file
            # LM Studio model IDs look like: qwen2.5-7b-instruct, llama-3.1-8b-instruct, etc.
            # Files are stored in ~/.lmstudio/models/publisher/repo/file.gguf

            lms_models_dir = Path.home() / ".lmstudio" / "models"
            deleted = False

            # Use the actual LM Studio ID we mapped from registry name
            model_id_to_delete = actual_model_name
            logger.info(f"Searching for LM Studio model ID: {model_id_to_delete}")

            # Search for matching model directory based on the model we want to delete
            # For qwen2.5:7b -> qwen2.5-7b-instruct, we need to find the matching repo
            for publisher_dir in lms_models_dir.iterdir():
                if not publisher_dir.is_dir():
                    continue
                for repo_dir in publisher_dir.iterdir():
                    if not repo_dir.is_dir():
                        continue

                    # Check if any GGUF files exist in this repo
                    gguf_files = list(repo_dir.glob("*.gguf"))
                    if gguf_files:
                        # Match by checking if the model_id matches this repo's pattern
                        # For qwen2.5-7b-instruct -> Qwen2.5-7B-Instruct-GGUF repo
                        repo_name_lower = repo_dir.name.lower()
                        model_id_lower = model_id_to_delete.lower().replace(':', '-')

                        # Strategy 1: Direct substring match (qwen2.5-7b-instruct in qwen2.5-7b-instruct-gguf)
                        if model_id_lower in repo_name_lower or repo_name_lower.replace('-gguf', '') == model_id_lower:
                            import shutil
                            shutil.rmtree(repo_dir)
                            logger.info(f"Deleted LM Studio model directory: {repo_dir}")
                            deleted = True
                            break

                        # Strategy 2: Match by normalized parts (handle qwen2.5 correctly)
                        # Remove dots, then split and compare
                        model_normalized = model_id_lower.replace('.', '')  # qwen25-7b-instruct
                        repo_normalized = repo_name_lower.replace('.', '')  # qwen25-7b-instruct-gguf
                        if model_normalized in repo_normalized:
                            import shutil
                            shutil.rmtree(repo_dir)
                            logger.info(f"Deleted LM Studio model directory: {repo_dir}")
                            deleted = True
                            break

                if deleted:
                    break

            if not deleted:
                logger.error(f"LM Studio model {model_name} (ID: {model_id_to_delete}) not found on disk")
                return {"success": False, "error": "Model files not found", "message": f"Model {model_name} files not found"}

            # Unload model from LM Studio to refresh its cache
            try:
                lms_path = Path.home() / ".lmstudio" / "bin" / "lms.exe"
                if lms_path.exists():
                    unload_result = subprocess.run(
                        [str(lms_path), "unload", model_id_to_delete],
                        capture_output=True,
                        text=True,
                        timeout=10
                    )
                    logger.info(f"Unloaded {model_id_to_delete} from LM Studio: {unload_result.stdout}")
            except Exception as e:
                logger.warning(f"Failed to unload model from LM Studio (non-critical): {e}")

            # Create a fake result object for unified handling below
            class Result:
                returncode = 0
                stdout = f"Deleted LM Studio model {model_name}"
                stderr = ""
            result = Result()

        if result.returncode == 0:
            logger.info(f"Successfully uninstalled model: {model_name}")

            # Check if this was the current model and switch to another if needed
            if hasattr(request.app.state, 'llm_client'):
                current_model = getattr(request.app.state.llm_client, 'model_name', '')
                if current_model == model_name:
                    # Get list of available models from BOTH providers
                    chat_models = []
                    embedding_models = ['nomic-embed-text', 'mxbai-embed-large', 'all-minilm']

                    # Try to find models from the SAME provider first
                    replacement_found = False

                    if provider == "ollama":
                        # Check Ollama for other models
                        try:
                            list_result = subprocess.run(
                                ["ollama", "list"],
                                capture_output=True,
                                text=True,
                                timeout=5
                            )
                            if list_result.returncode == 0:
                                lines = list_result.stdout.strip().split('\n')
                                for line in lines[1:]:  # Skip header
                                    parts = line.split()
                                    if parts:
                                        model_name_candidate = parts[0]
                                        is_embedding = any(embed in model_name_candidate.lower() for embed in embedding_models)
                                        if not is_embedding:
                                            chat_models.append(('ollama', model_name_candidate))
                        except Exception as e:
                            logger.error(f"Failed to check Ollama models: {e}")

                    elif provider == "lmstudio":
                        # Check LM Studio for other models
                        try:
                            async with httpx.AsyncClient(timeout=2.0) as client:
                                resp = await client.get("http://localhost:1234/v1/models")
                                if resp.status_code == 200:
                                    lms_models = [m['id'] for m in resp.json().get('data', [])]
                                    for lms_model in lms_models:
                                        is_embedding = any(embed in lms_model.lower() for embed in embedding_models)
                                        if not is_embedding:
                                            chat_models.append(('lmstudio', lms_model))
                        except Exception as e:
                            logger.error(f"Failed to check LM Studio models: {e}")

                    # If no models in same provider, check the other provider
                    if not chat_models:
                        if provider == "ollama":
                            # Try LM Studio
                            try:
                                async with httpx.AsyncClient(timeout=2.0) as client:
                                    resp = await client.get("http://localhost:1234/v1/models")
                                    if resp.status_code == 200:
                                        lms_models = [m['id'] for m in resp.json().get('data', [])]
                                        for lms_model in lms_models:
                                            is_embedding = any(embed in lms_model.lower() for embed in embedding_models)
                                            if not is_embedding:
                                                chat_models.append(('lmstudio', lms_model))
                            except Exception as e:
                                logger.error(f"Failed to check LM Studio as fallback: {e}")
                        else:
                            # Try Ollama
                            try:
                                list_result = subprocess.run(
                                    ["ollama", "list"],
                                    capture_output=True,
                                    text=True,
                                    timeout=5
                                )
                                if list_result.returncode == 0:
                                    lines = list_result.stdout.strip().split('\n')
                                    for line in lines[1:]:
                                        parts = line.split()
                                        if parts:
                                            model_name_candidate = parts[0]
                                            is_embedding = any(embed in model_name_candidate.lower() for embed in embedding_models)
                                            if not is_embedding:
                                                chat_models.append(('ollama', model_name_candidate))
                            except Exception as e:
                                logger.error(f"Failed to check Ollama as fallback: {e}")

                    if chat_models:
                        # Switch to first available chat model
                        new_provider, new_model = chat_models[0]

                        # Update llm_client with correct provider
                        if new_provider == "ollama":
                            request.app.state.llm_client.model_name = new_model
                            request.app.state.llm_client.base_url = "http://localhost:11434"
                            request.app.state.llm_client.api_style = "ollama"
                            os.environ['OLLAMA_MODEL'] = new_model
                            os.environ['ROAMPAL_LLM_OLLAMA_MODEL'] = new_model
                        else:
                            request.app.state.llm_client.model_name = new_model
                            request.app.state.llm_client.base_url = "http://localhost:1234"
                            request.app.state.llm_client.api_style = "openai"

                        # CRITICAL: Recycle HTTP client to use new base_url
                        if hasattr(request.app.state.llm_client, '_recycle_client'):
                            await request.app.state.llm_client._recycle_client()
                            logger.info(f"Recycled HTTP client for new provider: {new_provider}")

                        # Update .env file with locking
                        async with _env_file_lock:
                            env_path = Path(__file__).parent.parent.parent / '.env'
                            if env_path.exists():
                                lines = env_path.read_text().splitlines()
                                for i, line in enumerate(lines):
                                    if line.startswith('OLLAMA_MODEL='):
                                        lines[i] = f'OLLAMA_MODEL={new_model}' if new_provider == "ollama" else line
                                    elif line.startswith('ROAMPAL_LLM_OLLAMA_MODEL='):
                                        lines[i] = f'ROAMPAL_LLM_OLLAMA_MODEL={new_model}' if new_provider == "ollama" else line
                                env_path.write_text('\n'.join(lines) + '\n')

                        logger.info(f"Switched from uninstalled {provider} model to {new_provider} model: {new_model}")
                    else:
                        # No chat models available - warn user
                        logger.warning("No chat models available after uninstall. User must install a chat model.")
                        # Set to None to prevent embedding model usage
                        request.app.state.llm_client.model_name = None

            return {
                "success": True,
                "message": f"Successfully uninstalled {model_name}",
                "output": result.stdout
            }
        else:
            logger.error(f"Failed to uninstall model: {result.stderr}")
            return {
                "success": False,
                "error": result.stderr or "Unknown error occurred",
                "message": f"Failed to uninstall {model_name}"
            }
    except subprocess.TimeoutExpired:
        logger.error(f"Timeout while uninstalling model: {model_name}")
        return {
            "success": False,
            "error": "Operation timed out after 30 seconds",
            "message": f"Timeout uninstalling {model_name}"
        }
    except FileNotFoundError:
        logger.error("Ollama command not found")
        return {
            "success": False,
            "error": "Ollama is not installed or not in PATH",
            "message": "Ollama not found"
        }
    except Exception as e:
        logger.error(f"Error uninstalling model {model_name}: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": f"Error uninstalling {model_name}"
        }
# WebSocket endpoint for model installation (Tauri production)

@router.websocket("/pull-ws")
async def pull_model_websocket(websocket: WebSocket):
    """WebSocket endpoint for model downloads in Tauri production"""
    # Accept connection without origin validation (localhost only)
    try:
        await websocket.accept()
    except Exception as e:
        logger.error(f"WebSocket accept failed: {e}")
        return
    
    try:
        # Receive model name from client
        data = await websocket.receive_text()
        request_data = json.loads(data)
        model_name = request_data.get('model', '')
        
        if not model_name:
            await websocket.send_json({"type": "error", "message": "Model name is required"})
            await websocket.close()
            return
        
        # Validate model name
        if not _validate_model_name(model_name):
            await websocket.send_json({"type": "error", "message": "Invalid model name format"})
            await websocket.close()
            return
        
        # Check if already downloading
        async with _download_lock:
            if model_name in _downloading_models:
                await websocket.send_json({"type": "error", "message": f"Model {model_name} is already being downloaded"})
                await websocket.close()
                return
            _downloading_models.add(model_name)
        
        try:
            logger.info(f"WebSocket: Pulling model: {model_name}")
            await websocket.send_json({"type": "start", "model": model_name})
            
            # Use Ollama HTTP API for streaming
            async with httpx.AsyncClient(timeout=600.0) as client:
                async with client.stream(
                    "POST",
                    "http://localhost:11434/api/pull",
                    json={"name": model_name, "stream": True},
                    timeout=600.0
                ) as response:
                    async for line in response.aiter_lines():
                        # Check if download was cancelled
                        if _cancel_flags.get(model_name, False):
                            logger.info(f"WebSocket: Ollama download cancelled for {model_name}")
                            await websocket.send_json({"type": "error", "message": "Download cancelled by user"})
                            break

                        if line.strip():
                            try:
                                progress_data = json.loads(line)
                                status = progress_data.get("status", "")
                                
                                if "error" in progress_data:
                                    await websocket.send_json({
                                        "type": "error",
                                        "message": progress_data["error"]
                                    })
                                    break
                                
                                # Parse progress info
                                completed = progress_data.get("completed", 0)
                                total = progress_data.get("total", 0)
                                
                                if total > 0:
                                    percent = int((completed / total) * 100)
                                    downloaded_mb = f"{completed / (1024*1024):.1f} MB"
                                    total_mb = f"{total / (1024*1024):.1f} MB"
                                    
                                    await websocket.send_json({
                                        "type": "progress",
                                        "percent": percent,
                                        "downloaded": downloaded_mb,
                                        "total": total_mb,
                                        "message": f"{status}: {percent}%"
                                    })
                                else:
                                    await websocket.send_json({
                                        "type": "progress",
                                        "message": status
                                    })
                                
                                # Check if done
                                if status == "success" or progress_data.get("status") == "success":
                                    await websocket.send_json({"type": "complete", "model": model_name})
                                    logger.info(f"WebSocket: Successfully pulled {model_name}")
                                    break
                                    
                            except json.JSONDecodeError:
                                continue
            
        finally:
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)

        await websocket.close()

    except WebSocketDisconnect:
        logger.info(f"WebSocket: Client disconnected during download of {model_name}")
        _downloading_models.discard(model_name)
        _cancel_flags.pop(model_name, None)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        try:
            await websocket.send_json({"type": "error", "message": str(e)})
        except:
            pass
        _downloading_models.discard(model_name)
        _cancel_flags.pop(model_name, None)

@router.websocket("/download-gguf-ws")
async def download_gguf_websocket(websocket: WebSocket):
    """WebSocket endpoint for LM Studio GGUF downloads in Tauri production"""
    try:
        await websocket.accept()
    except Exception as e:
        logger.error(f"WebSocket accept failed: {e}")
        return

    model_name = None
    try:
        # Receive model name from client
        data = await websocket.receive_text()
        request_data = json.loads(data)
        model_name = request_data.get('model', '')

        if not model_name:
            await websocket.send_json({"type": "error", "message": "Model name is required"})
            await websocket.close()
            return

        # Validate model exists in mapping
        if model_name not in MODEL_TO_HUGGINGFACE:
            await websocket.send_json({"type": "error", "message": f"Model {model_name} not available for LM Studio auto-download"})
            await websocket.close()
            return

        model_info = MODEL_TO_HUGGINGFACE[model_name]

        # Check concurrency - use lock to prevent race conditions
        async with _download_lock:
            if model_name in _downloading_models:
                await websocket.send_json({"type": "error", "message": f"Model {model_name} is already being downloaded"})
                await websocket.close()
                return

            if _downloading_models:
                current = next(iter(_downloading_models))
                await websocket.send_json({"type": "error", "message": f"Cannot download {model_name}: {current} is downloading"})
                await websocket.close()
                return

            _downloading_models.add(model_name)

        try:
            # 1. Download GGUF file from HuggingFace
            cache_path = get_lmstudio_cache_path() / model_info['file']
            hf_url = f"https://huggingface.co/{model_info['repo']}/resolve/main/{model_info['file']}"

            logger.info(f"WebSocket: Downloading {model_name} from {hf_url}")
            await websocket.send_json({"type": "start", "model": model_name})
            await websocket.send_json({"type": "progress", "status": "downloading", "percent": 0, "message": "Starting download..."})

            async with httpx.AsyncClient(timeout=600.0, follow_redirects=True) as client:
                async with client.stream('GET', hf_url) as response:
                    response.raise_for_status()
                    total = int(response.headers.get('content-length', 0))
                    downloaded = 0
                    last_progress = 0

                    with open(cache_path, 'wb') as f:
                        async for chunk in response.aiter_bytes(chunk_size=8192):
                            # Check if download was cancelled
                            if _cancel_flags.get(model_name, False):
                                logger.info(f"WebSocket: LM Studio download cancelled for {model_name}")
                                await websocket.send_json({"type": "error", "message": "Download cancelled by user"})
                                break

                            f.write(chunk)
                            downloaded += len(chunk)

                            if total > 0:
                                percent = int((downloaded / total) * 100)
                                if percent >= last_progress + 1 or percent == 100:
                                    downloaded_str = format_bytes(downloaded)
                                    total_str = format_bytes(total)

                                    await websocket.send_json({
                                        "type": "progress",
                                        "status": "downloading",
                                        "percent": percent,
                                        "downloaded": downloaded_str,
                                        "total": total_str,
                                        "message": f"Downloading {percent}%"
                                    })
                                    last_progress = percent

            logger.info(f"WebSocket: Downloaded {model_name} to {cache_path}")

            # 2. Import via lms.exe
            await websocket.send_json({"type": "progress", "status": "importing", "percent": 100, "message": "Importing to LM Studio..."})

            lms_cli = get_lms_cli_path()
            if not lms_cli:
                raise Exception("lms.exe not found. Is LM Studio installed?")

            logger.info(f"WebSocket: Importing with lms.exe: {lms_cli}")

            # Use Popen with stdin PIPE to answer first-run prompt
            process = subprocess.Popen([
                str(lms_cli), "import",
                "--yes",
                "--copy",
                "--user-repo", model_info['repo'],
                str(cache_path)
            ], stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)

            stdout, stderr = process.communicate(input="Y\n", timeout=300)

            # Create result object to match expected interface
            class Result:
                def __init__(self, returncode, stdout, stderr):
                    self.returncode = returncode
                    self.stdout = stdout
                    self.stderr = stderr

            result = Result(process.returncode, stdout, stderr)

            # lms import sends warnings to stderr even on success, so check if file was actually imported
            # LM Studio copies files to ~/.lmstudio/models/user/repo/
            imported_file_path = Path.home() / ".lmstudio" / "models" / model_info['repo'].split('/')[0] / model_info['repo'].split('/')[1] / model_info['file']

            if not imported_file_path.exists():
                logger.error(f"lms import failed - file not found at expected location: {imported_file_path}")
                logger.error(f"lms stdout: {result.stdout}")
                logger.error(f"lms stderr: {result.stderr}")
                raise Exception(f"Import failed: Model file not found in LM Studio models directory")

            logger.info(f"WebSocket: Successfully imported {model_name} to {imported_file_path}")

            # Delete cache file after successful import to avoid duplicating disk usage
            try:
                cache_path.unlink()
                logger.info(f"Deleted cache file: {cache_path}")
            except Exception as e:
                logger.warning(f"Could not delete cache file {cache_path}: {e}")

            # Model is now available - LM Studio will auto-load on first use
            logger.info(f"WebSocket: Successfully imported {model_name} - ready for use")
            await websocket.send_json({"type": "loaded", "model": model_name, "message": "Model ready!"})

        finally:
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)

        await websocket.close()

    except WebSocketDisconnect:
        logger.info(f"WebSocket: Client disconnected during LM Studio download of {model_name}")
        if model_name:
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)
    except Exception as e:
        logger.error(f"WebSocket LM Studio error: {e}", exc_info=True)
        try:
            await websocket.send_json({"type": "error", "message": str(e)})
        except:
            pass
        if model_name:
            _downloading_models.discard(model_name)
            _cancel_flags.pop(model_name, None)
