# Benchmark Quick Start

Get your first benchmark results in 2 minutes.

## Setup

```bash
# Install benchmark dependencies
pip install -r benchmarks/requirements.txt
```

## Run Benchmarks

### Option 1: Quick Test (Fastest)
```bash
# Run one category to verify setup
pytest benchmarks/test_cold_start.py -v
```

### Option 2: Full Suite (Recommended)
```bash
# Run all benchmarks with detailed output
pytest benchmarks/ -v
```

### Option 3: Generate Report (Best for tracking)
```bash
# Run benchmarks and save report
python benchmarks/run_benchmarks.py --save-report
```

## Expected Output

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    ROAMPAL BENCHMARK RESULTS                          â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£

ğŸ“Š MEMORY SYSTEM PERFORMANCE:

  âœ“ Cold-Start Auto-Trigger
    â€¢ Hit Rate: 95.0%
    â€¢ Target: 100%
    â€¢ Status: âœ“ PASS

  âœ“ Memory Ranking Quality
    â€¢ Precision@5: 92.0%
    â€¢ Target: â‰¥90%
    â€¢ Status: âœ“ PASS

  âœ“ Outcome Tracking Accuracy
    â€¢ Accuracy: 88.0%
    â€¢ Target: â‰¥85%
    â€¢ Status: âœ“ PASS

  âœ“ Knowledge Graph Routing
    â€¢ Accuracy: 83.0%
    â€¢ Target: â‰¥80%
    â€¢ Status: âœ“ PASS

  âœ“ Books Search Recall
    â€¢ Recall@5: 87.0%
    â€¢ Target: â‰¥80%
    â€¢ Status: âœ“ PASS

  âœ“ Stale Data Resilience
    â€¢ Crash Rate: 0.0%
    â€¢ Target: 0%
    â€¢ Status: âœ“ PASS

â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  OVERALL SYSTEM GRADE: A (Excellent)                                  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

## What Each Test Does

**test_cold_start.py** (30 seconds)
- Tests auto-injection of user profile on message #1
- Verifies Content KG retrieval
- Tests fallback to vector search

**test_memory_ranking.py** (45 seconds)
- Tests importance-based ranking
- Verifies quality score (importance Ã— confidence) impact
- Measures precision@5 for high-importance facts

**test_outcome_tracking.py** (30 seconds)
- Tests score updates on worked/failed/partial outcomes
- Verifies score delta accuracy (Â±0.1)
- Tests score bounds enforcement

**test_kg_routing.py** (20 seconds)
- Tests knowledge graph query routing
- Verifies collection selection accuracy
- Tests LLM override capability

**test_books_search.py** (60 seconds)
- Tests book upload and semantic search
- Verifies content extraction (no empty results)
- Tests metadata preservation

**test_stale_data.py** (40 seconds)
- Tests KG cleanup on deletions
- Verifies fallback on stale data
- Tests zero-crash resilience

**Total runtime: ~3-4 minutes**

## Troubleshooting

### Import errors
```bash
# Make sure backend path is correct
export PYTHONPATH="${PYTHONPATH}:ui-implementation/src-tauri/backend"
```

### ChromaDB errors
```bash
# Benchmarks use isolated temp directories, shouldn't conflict with production
# If issues persist, stop Roampal and retry
```

### Test failures
```bash
# Run individual test with more detail
pytest benchmarks/test_cold_start.py::test_cold_start_injection_occurs -v -s
```

## Next Steps

1. **Track improvements**: Run benchmarks before/after changes
2. **Save baselines**: Use `--save-report` to track metrics over time
3. **Compare competitors**: See `docs/BENCHMARKS.md` for methodology
4. **Add custom tests**: Extend `benchmarks/` with your own tests

## Files Created

```
benchmarks/
â”œâ”€â”€ __init__.py                # Package init
â”œâ”€â”€ pytest.ini                 # Pytest configuration
â”œâ”€â”€ conftest.py                # Shared fixtures
â”œâ”€â”€ requirements.txt           # Test dependencies
â”œâ”€â”€ README.md                  # Documentation
â”œâ”€â”€ QUICKSTART.md              # This file
â”œâ”€â”€ run_benchmarks.py          # Benchmark runner
â”œâ”€â”€ test_cold_start.py         # Cold-start tests
â”œâ”€â”€ test_memory_ranking.py     # Ranking tests
â”œâ”€â”€ test_outcome_tracking.py   # Outcome tests
â”œâ”€â”€ test_kg_routing.py         # Routing tests
â”œâ”€â”€ test_books_search.py       # Books tests
â”œâ”€â”€ test_stale_data.py         # Resilience tests
â”œâ”€â”€ fixtures/                  # Test data (auto-created)
â””â”€â”€ reports/                   # Saved reports (auto-created)
```
